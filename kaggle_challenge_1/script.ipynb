{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a6ea7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Jigsaw - Agile Community Rules Classification\n",
    "# Advanced BERT Fine-tuning Solution\n",
    "# Target: 0.925+ AUC\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "import sys\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Starting Advanced BERT Solution...\", flush=True)\n",
    "print(f\"PyTorch version: {torch.__version__}\", flush=True)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\", flush=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80, flush=True)\n",
    "print(\"1. LOADING DATA\", flush=True)\n",
    "print(\"=\"*80, flush=True)\n",
    "\n",
    "train_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\", flush=True)\n",
    "print(f\"Test shape: {test_df.shape}\", flush=True)\n",
    "print(f\"Unique rules in train: {train_df['rule'].nunique()}\", flush=True)\n",
    "print(f\"Target distribution: {train_df['rule_violation'].value_counts().to_dict()}\", flush=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CREATE FORMATTED TEXT INPUT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80, flush=True)\n",
    "print(\"2. PREPARING STRUCTURED INPUT\", flush=True)\n",
    "print(\"=\"*80, flush=True)\n",
    "\n",
    "def create_input_text(row):\n",
    "    \"\"\"Create structured input that includes rule context and examples\"\"\"\n",
    "    text = f\"\"\"Rule: {row['rule']}\n",
    "\n",
    "Examples that VIOLATE this rule:\n",
    "- {row['positive_example_1']}\n",
    "- {row['positive_example_2']}\n",
    "\n",
    "Examples that DO NOT violate this rule:\n",
    "- {row['negative_example_1']}\n",
    "- {row['negative_example_2']}\n",
    "\n",
    "Comment to evaluate: {row['body']}\"\"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_simple_input(row):\n",
    "    \"\"\"Simpler format for faster processing\"\"\"\n",
    "    return f\"Rule: {row['rule']} | Positive: {row['positive_example_1']} {row['positive_example_2']} | Negative: {row['negative_example_1']} {row['negative_example_2']} | Comment: {row['body']}\"\n",
    "\n",
    "# Create both formats\n",
    "print(\"Creating structured inputs...\", flush=True)\n",
    "train_df['input_text'] = train_df.apply(create_simple_input, axis=1)\n",
    "test_df['input_text'] = test_df.apply(create_simple_input, axis=1)\n",
    "\n",
    "print(f\"Sample input:\\n{train_df['input_text'].iloc[0][:200]}...\", flush=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TRANSFORMER SETUP\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80, flush=True)\n",
    "print(\"3. SETTING UP TRANSFORMER MODEL\", flush=True)\n",
    "print(\"=\"*80, flush=True)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Choose model - using DeBERTa-v3 for best performance\n",
    "MODEL_NAME = 'microsoft/deberta-v3-small'  # Fast and effective\n",
    "# Alternative: 'microsoft/deberta-v3-base' for better performance but slower\n",
    "# Alternative: 'bert-base-uncased' for baseline\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\", flush=True)\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(\"✓ Tokenizer loaded\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\", flush=True)\n",
    "    print(\"Falling back to bert-base-uncased\", flush=True)\n",
    "    MODEL_NAME = 'bert-base-uncased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CREATE DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class RuleViolationDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "print(\"Creating datasets...\", flush=True)\n",
    "\n",
    "# Tokenize to check lengths\n",
    "sample_lengths = []\n",
    "for text in train_df['input_text'].head(100):\n",
    "    tokens = tokenizer(text, truncation=False)\n",
    "    sample_lengths.append(len(tokens['input_ids']))\n",
    "\n",
    "print(f\"Token length stats - Mean: {np.mean(sample_lengths):.0f}, Max: {np.max(sample_lengths):.0f}\", flush=True)\n",
    "\n",
    "# Set max length based on data\n",
    "MAX_LENGTH = min(512, int(np.percentile(sample_lengths, 95)) + 50)\n",
    "print(f\"Using max_length: {MAX_LENGTH}\", flush=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TRAINING WITH CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80, flush=True)\n",
    "print(\"4. TRAINING WITH CROSS-VALIDATION\", flush=True)\n",
    "print(\"=\"*80, flush=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.softmax(torch.tensor(predictions), dim=-1)[:, 1].numpy()\n",
    "    auc = roc_auc_score(labels, predictions)\n",
    "    return {'auc': auc}\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "N_FOLDS = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "oof_predictions = np.zeros(len(train_df))\n",
    "test_predictions = np.zeros(len(test_df))\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['rule_violation'])):\n",
    "    print(f\"\\n{'='*80}\", flush=True)\n",
    "    print(f\"FOLD {fold + 1}/{N_FOLDS}\", flush=True)\n",
    "    print(f\"{'='*80}\", flush=True)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_texts = train_df.iloc[train_idx]['input_text'].values\n",
    "    train_labels = train_df.iloc[train_idx]['rule_violation'].values\n",
    "    val_texts = train_df.iloc[val_idx]['input_text'].values\n",
    "    val_labels = train_df.iloc[val_idx]['rule_violation'].values\n",
    "    \n",
    "    train_dataset = RuleViolationDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "    val_dataset = RuleViolationDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_fold_{fold}',\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs_fold_{fold}',\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"auc\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training fold {fold + 1}...\", flush=True)\n",
    "    trainer.train()\n",
    "    \n",
    "    # Validation predictions\n",
    "    val_pred = trainer.predict(val_dataset)\n",
    "    val_probs = torch.softmax(torch.tensor(val_pred.predictions), dim=-1)[:, 1].numpy()\n",
    "    val_auc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Validation AUC: {val_auc:.4f}\", flush=True)\n",
    "    fold_scores.append(val_auc)\n",
    "    oof_predictions[val_idx] = val_probs\n",
    "    \n",
    "    # Test predictions\n",
    "    test_dataset = RuleViolationDataset(test_df['input_text'].values, None, tokenizer, MAX_LENGTH)\n",
    "    test_pred = trainer.predict(test_dataset)\n",
    "    test_probs = torch.softmax(torch.tensor(test_pred.predictions), dim=-1)[:, 1].numpy()\n",
    "    test_predictions += test_probs / N_FOLDS\n",
    "    \n",
    "    # Clean up\n",
    "    del model, trainer, train_dataset, val_dataset, test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. RESULTS AND SUBMISSION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80, flush=True)\n",
    "print(\"5. CROSS-VALIDATION RESULTS\", flush=True)\n",
    "print(\"=\"*80, flush=True)\n",
    "\n",
    "overall_auc = roc_auc_score(train_df['rule_violation'], oof_predictions)\n",
    "print(f\"\\nFold AUC scores: {[f'{s:.4f}' for s in fold_scores]}\", flush=True)\n",
    "print(f\"Mean Fold AUC: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})\", flush=True)\n",
    "print(f\"Overall OOF AUC: {overall_auc:.4f}\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80, flush=True)\n",
    "print(\"6. CREATING SUBMISSION\", flush=True)\n",
    "print(\"=\"*80, flush=True)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'].values,\n",
    "    'rule_violation': np.clip(test_predictions, 0.001, 0.999)\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n✓ Submission created successfully!\", flush=True)\n",
    "print(f\"\\nPrediction statistics:\", flush=True)\n",
    "print(f\"  Min: {submission['rule_violation'].min():.4f}\", flush=True)\n",
    "print(f\"  Max: {submission['rule_violation'].max():.4f}\", flush=True)\n",
    "print(f\"  Mean: {submission['rule_violation'].mean():.4f}\", flush=True)\n",
    "print(f\"  Median: {submission['rule_violation'].median():.4f}\", flush=True)\n",
    "\n",
    "print(\"\\nFirst 10 predictions:\", flush=True)\n",
    "print(submission.head(10), flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80, flush=True)\n",
    "print(\"TRAINING COMPLETE!\", flush=True)\n",
    "print(f\"Expected Public LB Score: ~{overall_auc:.4f}\", flush=True)\n",
    "print(\"=\"*80, flush=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
